<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Examining emerging capabilities and mitigating potential risks of VLLMs</title>
		<meta charset="utf-8" />
		<link rel="shortcut icon" href="https://aiisc.ai/kiwo-icwsm/images/ai_institute_logo.png" type="image/x-icon">
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<style>
		ul>li{
			margin-top: 2%;
			color: #000;
		}
		p{
			color: #000;
			text-align: justify;
		}
	</style>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<h2>
									<a href="index.html" class="logo"><strong>Examining emerging
										capabilities and mitigating
										potential risks of VLLMs</strong></a>

									</h2>
								
								</header>

							<!-- Content -->
								<section>
									<div style="font-weight: 800;font-size:medium">

										

									</div>
									
									<header class="main">
										<h2 id="abstract">ABSTRACT</h2>
									</header>

									<!-- <span style="background-color:black;" class="image main"><img src="./images/abstract.png"   alt="" /></span> -->

									<p>
										The emergence of very large language models
(VLLMs) has dramatically altered the trajectory of
progress in AI and its applications. With the release
of ChatGPT, that excitement has now transcended
boundaries from AI researchers to the common
man; indeed it asserts we are living in an exciting
time of scientific proliferation. However, we see two
divergent community views on VLLMs. Believers
in the magical powers of VLLMs claim that VLLMs
are autodidactic in learning a wide gamut of new
capabilities – referred to as "emerging capabilities"
in the community – an effect that gets pronounced
with model size and dataset scale. On the other
hand, critics are not yet prepared to acknowledge
the self-learning power of VLLMs; they criticize it
as only a statistical learner and out several flaws
– hallucination being the most prominent one. In
this forum, we want to bring together both com-
munities – the believers and the critics, to explore
exciting tasks together: (i) CT2 - Counter Turing
Test: AI-Generated Text Detection, and (ii) - HILT
HallucinatIon eLiciaTion through automatic detection and mitigation. Expect this to be an exciting
forum to discuss, debate, and explore exciting scientific pathways for the future.
									</p>
									<hr class="major" />

									<h2 id="schedule">1) Rationale - Why does AI need to be
										civilized? - Call for Papers (CFP)</h2>
									

										<p>
											Advances in AI during the past couple of years
have led to AI systems becoming immensely more
powerful than ever before. While their applications
for social good cannot be overstated, as an unintended by-product, risks of misuse have also been
exacerbated. This prompted an open petition letter
<a href="https://www.overleaf.com/project/641a4e37c41c86921947e0a9#cite.aihalt2023" target="_blank" rel="noopener noreferrer">(Marcus and of Life Institute, 2023)</a> (led by <a href="https://en.wikipedia.org/wiki/Gary_Marcus" target="_blank" rel="noopener noreferrer">Gary
Marcus</a>) by the nonprofit Future of Life Institute,
calling for all AI labs to immediately pause for at
least 6 months "moratorium" the training of AI systems more powerful than GPT-4. The letter has
(18K+ and still counting) signatures from technologists and luminaries, which include Yoshua Bengio,
Stuart Russell, Elon Musk, Steve Wozniak, and
Andrew Yang. It also includes policy leaders such
as Rachel Bronson, president of the Bulletin of
the Atomic Scientists, a science-oriented advocacy
group known for its warnings against humanity-ending nuclear war. On the other hand, the op-
posing campaign, which doesn’t believe in halting
scientific progress, has powerful people too, including Bill Gates (gat, 2023), Andrew Ng (aih, 2023),
Yann LeCun (aih, 2023) and many others. Furthermore, both the United States (whi, 2023) and the
European Union (eua, 2023) governments have
recently proposed regulatory frameworks for AI.
This is a significant time in the history of scientific development. In this forum, we will discuss
and debate emerging capabilities and mitigating
potential risks and limitations of VLLMs. Call for
papers includes, but is not limited to: • unique
emerging abilities of VLLM; • negative, position,
and full paper on potential risks of VLLMs; • ethics
and VLLMs; • making VLLMs more responsible;
• detection AI-generated content; • mitigation of
harmful hallucinations.
										</p>

									
									

									

				

					<h2 id="bios">2) Two Shared Tasks</h2>

					

					<p>
						Shared tasks are an effective way to attract re-
search attention to any emerging area. We will
host two shared tasks: (i) CT <sup>2</sup>, and (ii) HILT . The
findings of CT<sup>2</sup> will mitigate misusage risks, while HILT
endeavors to make VLLMs more humansensitive and responsible.
					</p>


					<h3>2.1) CT<sup>2</sup> - Counter Turing Test for
						AI-Generated Text Detection</h3>
						

						<p>
							With the emergence of ChatGPT, the risk of AI-
generated content has reached an alarming apoca-
lypse. ChatGPT has been declared banned by the
school system in NYC (Rosenblatt, 2023), Google
ads (Grant and Metz, 2022), and Stack Overflow
(Makyen and Olson, 1969), while scientific conferences like ACL (Chairs, 2023) and ICML (Foundation, 2023) have released new policies deterring
the usage of ChatGPT for scientific writing. After
the initial skepticism, ChatGPT has been seen as a
listed author in scientific papers (Kung et al., 2023;
O’Connor et al., 2022), while Elsevier (Elsevier,
2023) and Springer (Springer, 2023) have adopted
more inclusive guidelines on the use of ChatGPT
for scientific writing.
						</p>
						<p>
							Indeed, detecting AI-generated text has sud-
denly emerged as a concern that needs immediate
attention. While watermarking as a potential so-
lution to the problem is being studied by OpenAI
(Wiggers, 2022b), a handful of systems that detect
AI-generated text such as GPT-2 output detector
(Wiggers, 2022a), GLTR (Strobelt et al., 2022),
GPTZero (Tian, 2022), DetectGPT (Mitchell et al.,
2023), etc. have recently been orange observed in
practical use. To address the inevitable question
of ownership attribution for AI-generated artifacts,
the US Copyright Office (Office, 2023) released a
statement stating that if the content is traditional elements of authorship produced by a machine, the
work lacks human authorship and the office will not
register it for copyright. Given this cynosural spotlight on generative AI, AI-generated text detection
is a topic that needs a thorough investigation. In
this regard, there are three families of techniques
proposed so far:
						</p>

						<ul>
							<li>
								<strong>Watermarking :</strong> <span>
									First introduced in (Aaronson,
2022), watermarking AI-generated text involves
embedding an imperceptible code or signal to ver-
ify the author of a particular text with certainty.
(Kirchenbauer et al., 2023) proposed this by select-
ing the next token pseudorandomly (rather than
simply choosing the one with the highest probabil-
ity) using a cryptographic pseudorandom function
whose key is only possessed by the LLM maker.
It would be remiss not to mention the most obvious pitfall of this approach, which is that if the text
is altered or modified in any way, detecting the
watermark proves to be a difficult task.
								</span>
							</li>
							<li>
								<strong>
									Negative log likelihood (NLL) :
								</strong>
								<span>
									NLL-based implementations such as DetectGPT (Mitchell et al.,
2023) have demonstrated the detection of AI-generated text by comparing log-likelihood of generated tokens after perturbing the input text by
replacing some tokens with others. If the new,
perturbed version of the text lies in the negative
curvature regions of log-likelihood, it was likely
generated by AI. The limitation of this approach
is that it requires access to the log probabilities of
the text in order to work which implies that knowledge of which LLM was used to generate the text
is essential;
								</span>
							</li>
							<li><strong>Perplexity and Burstiness:</strong>
								<span>
									GPTZero (Tian,
2022), an example of a detection technique based
on perplexity and burstiness, has demonstrated
that a text with lower perplexity (a measure of how
predictable the text is), and with lower burstiness
(the measure of how uniform text is) has a high
probability of being generated by an AI. The limitations here are that GPTZero also requires access
to log probabilities of text as well as the fact that
it approximates perplexity values using a linear
model.

								</span><br>
								<span>
									Although AI-generated text detection has suddenly received immense attention, Liang et al.
(Liang et al., 2023) suggest that available AI-generated text detectors consistently misclassify non-
native English writing samples as AI-generated,
whereas native writing samples are accurately
identified, highlighting the ethical implications of deploying AI-generated content detectors and risking
misrepresentation. This implies that a community
effort is needed to tackle the issue of detectors
penalizing under-represented sub-population(s).
In our recent publication, "Counter Turing Test
CT 2: AI-Generated Text Detection Challenges"
(Chakraborty et al., 2023), we introduce a benchmark for evaluating the robustness of AGTD techniques. Our results clearly show the vulnerabilities
of current AGTD methods. As discussions on AI
policy intensify, assessing the detectability of content produced by LLMs is vital. To this end, we
present the AI Detectability Index (ADI) for quantitative ranking based on detectability.
								</span>
								<br>
								<span>The CT<sup>2</sup>  task will be the first of its kind in bringing together researchers in advancing the area of
									detecting AI-assisted generated text.</span>
							</li>
						</ul>
						
						<h4>
							2.1.1)  Data to be released and the task
						</h4>
					<p>
						CT<sup>2</sup> will consist of three sub-tasks. We will be
releasing 100K data points, consisting of (i) prompt,
(ii) human-written text, and (iii) AI-generated text
by 15 different LLMs.
					</p>
					<ul>
						<li><strong>Task A : </strong>
						given a set of human-generated text documents vs. AI-generated text documents participants need to design techniques to detect AI-generated text. Indeed, human-written text vs.
AI-generated text would be parallel, which means
they will be on the same topic. In this task, we
will let participants know that the generated text is
from GPT, OPT, BERT, XLNet, etc. As such, this
is an LLM-specific AI detection task.
</li>
<li><strong>Task B:</strong>
	 in this task, we will not tell people which
the generated text is using which LLM. Participants need to design techniques which is LLM
agnostic.
</li>
<li>
	<strong>Task C:</strong>
	 In this task, we will offer AI-assisted writing, i.e., AI-generated text interlaced with minor
	edits by another language model and human, as
	input. Given the intricacies and challenges of AI-assisted writing, it would be the hardest task to
	attempt.
	
</li>
</ul>

<h3>
	2.2) HILT: HallucinatIon eLiciaTion
through automatic detection and
mitigation
</h3>

<p>
	With the recent and rapid advances in the areas
of LLMs and Generative AI, the pre-eminent and
ubiquitous concern is of hallucination. We release
large-scale first-of-its-kind human-annotated data
with detailed annotations on - intrinsic vs. extrinsic hallucinations, and degree of hallucination, and
we ask participants to come up with either black-box factuality Assessment and/or evidence-based
fact-checking. First, we define categories of hallucinations:
</p>

<ul>
	<li>
		<strong>Intrinsic Hallucination:</strong> Intrinsic hallucination
			refers to the phenomenon when an LLM gener-
			ates text that topically slightly deviates from the
			input and/or has a lack of grounding in reality. For
			example, given a prompt “USA on Ukraine war ”
			an LLM generates “U.S. President Barack Obama
			says the U.S. will not put troops in Ukraine”. We
			can see a clear case of intrinsic hallucination as
			the US president during the Ukraine-Russia war is
			Joe Biden, not Barack Obama, contradicting the
			reality.
	</li>
	<li>
	<strong>Extrinsic Hallucination : </strong>	We define extrinsic hal-
lucination to be the generated output from an LLM
that cannot be verified, or contradicted from the
source content provided as prompt. For exam-
ple, we provide a prompt stating “North Korea has
conducted six underground nuclear tests, and a
seventh may be on the way.”, and the resulting output generated by the model was “The international
community has condemned North Korea’s nuclear
tests, with the UN Security Council imposing a
range of sanctions in response. The US and other
world powers have urged North Korea to abandon
its nuclear weapons program and to comply with
international law.” As the source input makes no
reference to the U.S. or U.N. Security Council imposing any sanctions, the claimed imposition of
sanctions cannot be verified from the input alone.
	</li>

</ul>

					<div>

						<h4>Usha Lokala:</h4>

						<div style="display: flex;gap:30px">

							<p>
								<div class="image">
									<img src="images/Usha.jpg" height="150" alt="" />
								</div>
								<p>
									
									She is a Ph.D. student at AIISC. Her research interests include ontology engineering, knowledge graphs and natural
language processing. Her interdisciplinary research funded by NIH,
NIDA, and NSF applies ontology, deep learning, and natural language processing in the domain of Public Health, Addiction, Social
Media Analysis and Age related Cognition. Her work has been
published in reputed conferences and Journals (IEEE, Drug and Alcohol Dependence, WWW, CPDD, AAAI ICWSM, JMIR, PLOS One).
Lokala’s work [6] on public health addictions won second prize
in Opioid Challenge at SBP BRiMS 2018, a computational social
science conference. She also hosted tutorials and talks at venues
AAAI ICWSM, AI ML Systems, ASONAM, IJCAI Special Edition
on Responsible Social Media Mining. For more details, please visit
<a href="https://www.linkedin.com/in/usha-lokala/" target="_blank" rel="noopener noreferrer">Lokala’s webpage</a>.
																</p>
							</p>

						</div>
						<hr>
						<br>

						<h4>Kaushik Roy:</h4>
						<div style="display: flex;gap:30px">

							<p>
								<div class="image">
									<img src="images/Kaushik.jpg" height="150" alt="" />
								</div>
								<p>
								
									He is a Ph.D. student at AIISC. He completed his
master’s in computer science at Indiana University Bloomington
and has worked at UT Dallas’s starling lab. His research interests
include statistical relational artificial intelligence, sequential decision making, knowledge graphs, and reinforcement Learning. His
work is published in reputed conferences in IEEE, KR, AAAI, and
ECML-PKDD. He also hosted several tutorials at AAAI ICWSM,
KGC, and AI ML Systems Conferences. For more details, please visit
<a href="https://www.linkedin.com/in/kaushik-roy-b8a323ab/" target="_blank" rel="noopener noreferrer">Roy’s webpage</a>.
									
								</p>
							</p>

						</div>
						<hr>
						<br>
						
						<h4>Utkarshani Jaimini:</h4>

						<div style="display: flex;gap:30px">

							<p>
								<div class="image">
									<img src="./images/Utakarsini_2.jpg" height="150" alt="" />
								</div>
								<p>
									
									She is a Ph.D. student at AIISC. Her research focuses on developing a richer representation of causality
using knowledge graph (CausalKG) for better explainability with
the applications in autonomous driving and healthcare. Her research
interests includes causal analysis, applied machine learning to solve
healthcare problems, predictive analytic, information extraction,
augmented personalized health, internet of things (IoTs), semantic
web and semantic cognitive perceptual computing. Her work is
published in reputed conferences and journals (ISWC, ML4H, IC2S2,
IEEE Internet Computing, JMIR, American Thoracic Society, SLEEP
Society meeting). For more details, please visit <a href="https://www.linkedin.com/in/utkarshanijaimini/" target="_blank" rel="noopener noreferrer">Jaimini’s webpage</a>.
																</p>
							</p>

						</div>

						<hr>
						<br>
						<h4>Amit Sheth:</h4>

						<div style="display: flex;gap:30px">

							<p>
								<div class="image">
									<img src="images/Amit.jpg" height="150" alt="" />
								</div>
								<p>
									
									He is an Educator, Researcher, and Entrepreneur. He
									is the founding director of AIISC and NCR Chair at The University
									of South Carolina. Previously , he was the Lexis-Nexis Ohio Eminent Scholar and the executive director of Ohio Center of Excellence
									in Knowledge-enabled Computing. He is a Fellow of IEEE, AAAI,
									ACM, and AAAS. He has organized more then 100 international events (general/program chair, organization committee chair), more then 70 keynotes,
									given more then 45 many well-attended tutorials and is among the well-cited
									computer scientists. He has founded three companies by licensing
									his university research outcomes, including the first Semantic Web
									company in 1999 that pioneered technology similar to what is found
									today in Google Semantic Search and Knowledge Graph. Several
									commercial products and deployed systems have resulted from his
									research. For more details, please visit <a href="http://amit.aiisc.ai/" target="_blank" rel="noopener noreferrer">Dr. Sheth’s webpage</a>.
																</p>
							</p>

						</div>
						<br>
						<br>
						<hr>
						<br>
						<br>

						<h2 id="topic">3 TOPIC AND RELEVANCE</h2>
						<p>
							SocMedia emerged as the go-to platform for mental health (MH)
support for patients seeking Mental Health Care (MHCare). The
transition of patients from clinical settings to SocMedia has been
facilitated by peer-support groups and the lack of social stigma. For
example, in order to find clues that can demonstrate a correlation or
cause between various MH illnesses and a patient’s propensity for
suicide, researchers have begun examining SocMedia content. In
that case, this tutorial is relevant to develop causal AI systems for a
deeper understanding of SocMedia conversations that may result in
policy decisions due to the abundance of SocMedia conversations
in the areas of health care, mental health, substance use, etc. that
are available on the internet whereas surface level deep learning
systems cannot achieve such goals.
						</p>

						<br>
						<br>
						<hr>
						<br>
						<br>


						
					<h2 id="audience">4 TUTORIAL AUDIENCE AND PREREQUISITES</h2>
					<p>
						Researchers from academia, business, and healthcare professionals can participate in this tutorial to discuss the intersection of
causal representation, reasoning, semantic linking, NLP, and deep
learning. Since the tutorial is taught in a lecture-style format, a
fundamental knowledge of causal systems, Knowledge Graph (KG),
Ontology, and natural language processing (NLP) is preferred. This
will make it possible for the public to understand the limitations
of statistical AI and to monitor the development of AI as it moves
toward neuro-symbolic AI. The lesson will include enough examples
and use cases of causal AI systems and techniques. The present state
of AI/ML systems for web based analysis of health care will be explained to newcomers interested in these systems. The methodology
and datasets provided in the tutorial will be valued by experienced
participants as viable solutions to common technical challenges in
the social good domains. We welcome additional studies using our
datasets and methodology from other researchers in order to deepen
our understanding of causal AI systems for web and health care.
					</p>
						
						<br>
						<br>
						<hr>
						<br>
						<br>
						<h2 id="related">5 RELATED TUTORIALS, AND TALKS</h2>
						<p>Our tutors have co-presented multiple tutorials as follows:</p>

						<ul>
							<li>
								<b>
									Talk:
								</b>
								<span>
									Causal Knowledge Graph Explainability using Interventional and Counterfactual reasoning <a href="https://github.com/Imperfect-Knowledge/ik2022" target="_blank" rel="noopener noreferrer">(webpage link)</a>.
								</span>
							</li>
							<li>
								<b>
									Tutorial :
								</b>
								<span>
									Neuro-symbolic AI for Mental Healthcare <a href="https://aiisc.ai/neurone/" target="_blank" rel="noopener noreferrer"> (webpage link)</a>
								</span>
							</li>
							<li>
						<p>
							Knowledge-infused Reinforcement Learning <a href="https://aiisc.ai/kirl/" target="_blank" rel="noopener noreferrer">(webpage link)</a> 
						</p>
							</li>
							<li>
								<p>
									Explainable AI using Knowledge Graphs <a href="https://aiisc.ai/xaikg/" target="_blank" rel="noopener noreferrer">(webpage link)</a> 
								</p>
							</li>
							<li>
								<p>
									Knowledge In-Wisdom Out-Explainable Data for AI in Cyber
Social Threats and Public Health <a href="https://aiisc.ai/kiwo-icwsm/" target="_blank" rel="noopener noreferrer">(webpage link)</a> 
								</p>
								<p>
									Knowledge-infused Deep Learning <a href="http://kidl2020.aiisc.ai/" target="_blank" rel="noopener noreferrer">(webpage link)</a>
								</p>
							</li>
							<li>
								<p>
									Knowledge-infused NLU for Addiction and Mental Health Research
								</p>
							</li>
						</ul>



							<br>
							<br>
							<hr>
							<br>
							<br>
							<h2 id="societal"> 6 TUTORIAL SOCIETAL IMPACTS</h2>
							<p>

							
							Through the development of domain-specific data sets, cuttingedge frameworks, and computational methods for comprehending user language and asynchronous conversations on
a variety of platforms, we investigated a new healthcare dimension of SocMedia like Mental Health Care, and User interactions. Through the use of these methods, we hope to
(a) build the new clinical process guidelines for patients and
develop task-based Order Sets (b) create actionable Order Sets
that are integrated into the system and aid medical professionals in making choices and avoiding delays and discrepancies
in diagnosis and treatment. (c) create process-guided explanation models that are simple for the end-user to comprehend
(d) create causal AI system using a knowledge-graph-based
methodology for better explainability, support for intervention and counterfactuals in social good domains.
</p>
								<br>
								<br>
								<hr>
								<br>
								<br>
								
								<h2 id="slides">
									7 SLIDES
								</h2>
							
								<iframe width="100%" height="600px" src="https://drive.google.com/file/d/1aps7kH4P9TvTOQtty-KIVF347Skdj55C/preview" width="640" height="480" allow="autoplay"></iframe>
								<br>
								<br>
								<hr>
								<br>
								<br>

								<h2 id="slides">
									8 VIDEO
								</h2>
							
								<iframe width="100%" height="600px" src="https://www.youtube.com/embed/4-q1QKq5kvU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
								<br>
								<br>
								<hr>
								<br>
								<br>


								
						<h2 id="reference">REFERENCES</h2>

						<ul style="list-style-type:none;">
							<li>
								[1] Manas Gaur, Ankit Desai, Keyur Faldu, and Amit Sheth. 2020. Explainable AI Using
								Knowledge Graphs. In ACM CoDS-COMAD Conference.
							</li>

<li>
	[2] Manas Gaur, Ugur Kursuncu, Amit Sheth, Ruwan Wickramarachchi, and Shweta
	Yadav. 2020. Knowledge-infused deep learning. In Proceedings of the 31st ACM
	Conference on Hypertext and Social Media. 309–310.
</li>

<li>
	[3] Utkarshani Jaimini and Amit Sheth. 2022. CausalKG: Causal Knowledge Graph
	Explainability using interventional and counterfactual reasoning. IEEE Internet
	Computing 26, 1 (2022), 43–50.
</li>

<li>
	[4] U Lokala, M Gaur, K Roy, and A Sheth. 2021. Communication: Knowledge-infused
	NLU for Addiction and Mental Health Research. In ASONAM 2021- IEEE ACM
	International Conference on Advances in Social Networks Analysis and Mining. The
	Hague.
</li>

<li>
	[5] U Lokala, M Gaur, K Roy, and A Sheth. 2021. Talk: Knowledge-infused Natural
Language understanding for Public Health, Epidemiology, Substance Use, and
Mental Health. In Workshop on Mining Actionable Insights from Social Networks.
15th International Joint Conference on Artificial Intelligence.


</li>

<li>
	[6] Usha Lokala, Francois R Lamy, Raminta Daniulaityte, Amit Sheth, Ramzi W Nahhas,
Jason I Roden, Shweta Yadav, and Robert G Carlson. 2019. Global trends, local
harms: availability of fentanyl-type drugs on the dark web and accidental overdoses
in Ohio. Computational and mathematical organization theory 25, 1 (2019), 48–59.

</li>

<li>
	[7] Kaushik Roy, Manas Gaur, Qi Zhang, and Amit Sheth. 2022. Tutorial: Knowledgeinfused Reinforcement Learning. The Knowledge Graph Conference (2022).

</li>

<li>
	[8] Kaushik Roy, Usha Lokala, Manas Gaur, and Amit Sheth. 2022. Tutorial: Neurosymbolic AI for Mental Healthcare. International Conference on AI-ML Systems
	(2022).
</li>

<li>
	[9] Amit Sheth, Kaushik Roy, Manas Gaur, and Usha Lokala. 2021. Tutorial on Knowledge In-Wisdom Out-Explainable Data for AI in Cyber Social Threats and Public
	Health. AAAI ICWSM (2021).
	
</li>

						</ul>



						
					</div>

				
					

					
					
			

												

										
								</section>


						</div>
					</div>



				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<!-- <section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section> -->

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<span style="background-color:#f5f6f7;" class="image main"><img src="https://aiisc.ai/assets/img/logo.png"   alt="" /></span>

									<ul>
										<li><a href="#abstract">ABSTRACT</a></li>
										<li><a href="#schedule">1 TUTORIAL SCHEDULE AND ACTIVITIES</a></li>
										<li><a href="#bios">2 TUTORIAL ORGANIZER BIOS</a></li>
										<li><a href="#topic">3 TOPIC AND RELEVANCE</a></li>

										<li><a href="#audience">4 TUTORIAL AUDIENCE AND PREREQUISITES</a></li>
										<li><a href="#related">5 RELATED TUTORIALS, AND TALKS</a></li>
										<li><a href="#societal">6 TUTORIAL SOCIETAL IMPACTS</a></li>
										<li><a href="#slides">7 SLIDES</a></li>
										<li><a href="#reference">REFERENCES</a></li>

										
									</ul>
								</nav>

					


						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>